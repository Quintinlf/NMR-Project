{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2cc2b5",
   "metadata": {},
   "source": [
    "<!-- New Markdown Cell: Model Selection & Î”SNR Summary -->\n",
    "\n",
    "# ðŸ§ª Checkpoint Model Selection & Î”SNR Assessment\n",
    "\n",
    "### ðŸŽ¯ Goal  \n",
    "Identify which checkpoint is most (over)fit to the `krishna/` dataset while also computing the average Î”SNR across a broader `general/` dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‚ Data Layout (assumed)\n",
    "- `checkpoints/` â†’ multiple model folders or files (e.g. `epoch_*.pt` / `model_*.ckpt`)\n",
    "- `data/krishna/` â†’ targeted evaluation set (possible overfitting detection)\n",
    "- `data/general/` â†’ broader evaluation set\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Metrics\n",
    "| Metric | Krishna | General | Notes |\n",
    "|--------|---------|---------|-------|\n",
    "| Loss (val) | ... | ... | Lower may indicate overfit if gap large |\n",
    "| SNR_raw | ... | ... | Baseline signal-to-noise |\n",
    "| SNR_model | ... | ... | After model enhancement |\n",
    "| Î”SNR = SNR_model âˆ’ SNR_raw | ... | ... | Use mean over samples |\n",
    "| Î”SNR (avg) | ... | ... | Compare across checkpoints |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Î”SNR Computation\n",
    "For each sample i:  \n",
    "Î”SNRáµ¢ = SNR_modeláµ¢ âˆ’ SNR_rawáµ¢  \n",
    "Aggregate: mean(Î”SNRáµ¢), std(Î”SNRáµ¢)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Overfitting Heuristics\n",
    "- Very low Krishna loss + relatively poorer General Î”SNR â†’ likely overfit.\n",
    "- Large gap: (Krishna Î”SNR âˆ’ General Î”SNR) â†’ potential over-specialization.\n",
    "- Checkpoint preference: maximize General Î”SNR while keeping Krishna performance acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸªœ Procedure\n",
    "1. Enumerate checkpoints.\n",
    "2. For each: load model, run inference on Krishna & General datasets.\n",
    "3. Compute raw SNR (pre-enhancement) and enhanced SNR.\n",
    "4. Store:\n",
    "   - val_loss (if available)\n",
    "   - mean Î”SNR (Krishna)\n",
    "   - mean Î”SNR (General)\n",
    "5. Rank primarily by General Î”SNR; flag overfit if Krishna >> General.\n",
    "6. Select: Highest General Î”SNR with smallest Krishnaâˆ’General gap (or according to project goals).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Output Suggestion\n",
    "Produce a sorted table:\n",
    "Checkpoint | val_loss | Î”SNR_Krishna | Î”SNR_General | Gap | Overfit_Flag\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§· Notes\n",
    "- Ensure consistent SNR formula: SNR = Î¼_signal / Ïƒ_noise (or 20Â·log10 if amplitude-based).\n",
    "- Normalize input if models expect standardized tensors.\n",
    "- Use same random seed for reproducibility.\n",
    "- Optionally add violin plots for Î”SNR distributions.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ›  Next Cell: Runner\n",
    "A Python cell will implement this evaluation loop.\n",
    "\n",
    "#### Depending upon the particular overfitting of the data I might have to use multiple models for multiple spectra..... if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc19848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint evaluation and Î”SNR computation.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from pathlib import Path\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# ...existing code...\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "KRISHNA_DIR = Path(\"data/krishna\")\n",
    "GENERAL_DIR = Path(\"data/general\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def list_checkpoints(path: Path) -> List[Path]:\n",
    "    return sorted([p for p in path.iterdir() if p.suffix in (\".pt\", \".ckpt\")])\n",
    "\n",
    "def load_model(cp: Path):\n",
    "    model = torch.load(cp, map_location=DEVICE)\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    return model\n",
    "\n",
    "def load_dataset(folder: Path) -> List[torch.Tensor]:\n",
    "    tensors = []\n",
    "    for f in sorted(folder.glob(\"*.npy\")):\n",
    "        arr = np.load(f)\n",
    "        tensors.append(torch.tensor(arr, dtype=torch.float32, device=DEVICE))\n",
    "    return tensors\n",
    "\n",
    "def compute_snr(raw: torch.Tensor, enhanced: torch.Tensor) -> Tuple[float,float,float]:\n",
    "    # Simple SNR: mean(signal)/std(noise_residual)\n",
    "    signal_mean = raw.abs().mean().item()\n",
    "    noise_residual = (raw - enhanced)\n",
    "    noise_std = noise_residual.std().item() + 1e-12\n",
    "    snr_raw = signal_mean / (raw.std().item() + 1e-12)\n",
    "    snr_enh = signal_mean / noise_std\n",
    "    return snr_raw, snr_enh, snr_enh - snr_raw\n",
    "\n",
    "def evaluate_checkpoint(model, data: List[torch.Tensor]) -> Dict[str, float]:\n",
    "    deltas = []\n",
    "    for x in data:\n",
    "        with torch.no_grad():\n",
    "            y = model(x.unsqueeze(0))\n",
    "            if isinstance(y, (list, tuple)):\n",
    "                y = y[0]\n",
    "            y = y.squeeze(0)\n",
    "        snr_raw, snr_enh, delta = compute_snr(x, y)\n",
    "        deltas.append(delta)\n",
    "    return {\n",
    "        \"delta_mean\": mean(deltas),\n",
    "        \"delta_std\": stdev(deltas) if len(deltas) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "def try_val_loss(model) -> float:\n",
    "    return getattr(model, \"val_loss\", float(\"nan\"))\n",
    "\n",
    "def main():\n",
    "    checkpoints = list_checkpoints(CHECKPOINT_DIR)\n",
    "    krishna_data = load_dataset(KRISHNA_DIR)\n",
    "    general_data = load_dataset(GENERAL_DIR)\n",
    "\n",
    "    rows = []\n",
    "    for cp in checkpoints:\n",
    "        model = load_model(cp)\n",
    "        kr = evaluate_checkpoint(model, krishna_data)\n",
    "        ge = evaluate_checkpoint(model, general_data)\n",
    "        val_loss = try_val_loss(model)\n",
    "        gap = kr[\"delta_mean\"] - ge[\"delta_mean\"]\n",
    "        rows.append({\n",
    "            \"checkpoint\": cp.name,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"delta_krishna\": kr[\"delta_mean\"],\n",
    "            \"delta_general\": ge[\"delta_mean\"],\n",
    "            \"gap\": gap\n",
    "        })\n",
    "\n",
    "    # Ranking by general Î”SNR descending\n",
    "    rows.sort(key=lambda r: r[\"delta_general\"], reverse=True)\n",
    "\n",
    "    print(f\"{'Checkpoint':30} {'val_loss':>10} {'Î”SNR_K':>10} {'Î”SNR_G':>10} {'Gap':>10}\")\n",
    "    for r in rows:\n",
    "        print(f\"{r['checkpoint']:30} {r['val_loss']:10.4f} {r['delta_krishna']:10.4f} {r['delta_general']:10.4f} {r['gap']:10.4f}\")\n",
    "\n",
    "    best = rows[0] if rows else None\n",
    "    if best:\n",
    "        print(\"\\nSelected (tentative) best (general Î”SNR):\", best['checkpoint'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6c069",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
